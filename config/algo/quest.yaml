defaults:
  - base
  - _self_

policy:
  _target_: quest.algos.quest.QueST
  autoencoder:
    _target_: quest.algos.quest_modules.skill_vae.SkillVAE
    action_dim: ${task.shape_meta.action_dim}
    encoder_dim: 256
    decoder_dim: 256
    skill_block_size: ${algo.skill_block_size}
    downsample_factor: ${algo.downsample_factor}

    encoder_heads: 4
    encoder_layers: 2
    decoder_heads: 4
    decoder_layers: 4

    attn_pdrop: 0.1
    use_causal_encoder: true
    use_causal_decoder: true

    vq_type: "fsq" # "vq" or "fsq"
    # fsq_level: [8,5,5,5]
    fsq_level: null
    codebook_dim: 512 # only used for vq
    codebook_size: ${algo.codebook_size} # if fsq level is null then it will automatically compute it according to this
  policy_prior:
    _target_: quest.algos.quest_modules.skill_gpt.SkillGPT
    action_dim: ${task.shape_meta.action_dim}
    start_token: 1000 # TODO: leaving this here to keep things working. need to 
    offset_layers: 0
    offset_hidden_dim: 512
    offset_dim: ${eval:'${task.shape_meta.action_dim} * ${algo.skill_block_size}'}
    vocab_size: 1000
    block_size: ${eval:'${algo.skill_block_size} // ${algo.downsample_factor}'}
    n_layer: 6
    n_head: 6
    n_embd: ${algo.policy_prior_hidden_dim}
    attn_pdrop: 0.1
    embd_pdrop: 0.1
    beam_size: 5 # value of k for top k sampling
    temperature: 1.0 # temperature for sampling
    device: ${device}
  image_encoder_factory:
    _target_: quest.algos.utils.rgb_modules.ResnetEncoder
    _partial_: true
    output_size: ${algo.obs_emb_dim}
    pretrained: false
    freeze: false
    remove_layer_num: 4
    no_stride: false
    language_fusion: 'none'
  lowdim_encoder_factory:
    _target_: quest.algos.utils.mlp_proj.MLPProj
    _partial_: true
    output_size: ${algo.proprio_emb_dim}
    num_layers: 1
  obs_proj:
    _target_: quest.algos.utils.mlp_proj.MLPProj
    input_size: ${eval:'${algo.obs_emb_dim} * len(${task.shape_meta.observation.rgb}) + ${algo.proprio_emb_dim} * len(${task.shape_meta.observation.lowdim})'}
    output_size: ${algo.policy_prior_hidden_dim}
  task_encoder:
    task_embedding_type: ${algo.task_embedding_type}
    num_embeddings: ${task.n_tasks}
    input_dim: 512 # output of pretrained lang encoder (CLIP here)
    embedding_dim: ${algo.policy_prior_hidden_dim}
  # TODO: this assumes that all images have the same shape
  image_aug_factory:
    _target_: quest.algos.utils.data_augmentation.DataAugGroup
    _partial_: true
    aug_list:
      - _target_: quest.algos.utils.data_augmentation.BatchWiseImgColorJitterAug
        _partial_: true
        brightness: 0.3
        contrast: 0.3
        saturation: 0.3
        hue: 0.3
        epsilon: 0.1
      - _target_: quest.algos.utils.data_augmentation.TranslationAug
        _partial_: true
        translation: 4
  loss_fn:
    _target_: torch.nn.L1Loss
  optimizer_factory:
    _target_: torch.optim.AdamW
    _partial_: true
    lr: ${algo.lr}
    betas: [0.9, 0.999]    
    weight_decay: ${algo.weight_decay}
  scheduler_factory:
    _target_: torch.optim.lr_scheduler.CosineAnnealingLR
    _partial_: true
    eta_min: 1e-5
    last_epoch: -1
    T_max: ${training.n_epochs}
  stage: ${stage}
  l1_loss_scale: ${algo.l1_loss_scale}
  action_horizon: ${algo.action_horizon}
  shape_meta: ${task.shape_meta}
  device: ${device}

name: quest

# Put hyperparameters that are susceptible to change here
lr: 0.0001
weight_decay: 0.0001

policy_prior_hidden_dim: 384

proprio_emb_dim: 128
obs_emb_dim: 256 # from resnet_out_dim to this value using MLP
codebook_size: 1024

l1_loss_scale: 0

action_horizon: 8 # how many predicted actions to execute

frame_stack: 1
skill_block_size: 32 # this is input sequence length to encoder
downsample_factor: 4

dataset:
  seq_len: ${algo.skill_block_size}
  frame_stack: ${algo.frame_stack}
  obs_seq_len: 1
  load_obs_for_pretrain: false