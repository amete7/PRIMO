policy:
  _target_: quest.algos.quest.QueST
  autoencoder:
    _target_: quest.algos.quest_modules.skill_vae.SkillVAE
    action_dim: 4
    encoder_dim: 256
    decoder_dim: 256
    skill_block_size: ${algo.skill_block_size}

    encoder_heads: 4
    encoder_layers: 2
    decoder_heads: 4
    decoder_layers: 4

    attn_pdrop: 0.1
    use_causal_encoder: true
    use_causal_decoder: true

    vq_type: "fsq" # "vq" or "fsq"
    fsq_level: [8,5,5,5]
    codebook_dim: 512 # only used for vq
    codebook_size: 1024 # only used for vq

    kernel_sizes: [5,3,3] # conv module will have 3 layers with kernel sizes 5,3,3
    strides: [1,1,1] # conv module will have 3 layers with strides 2,2,1
  policy_prior:
    _target_: quest.algos.quest_modules.skill_gpt.SkillGPT
    start_token: 1000
    offset_layers: 0
    offset_hidden_dim: 512
    offset_dim: ${eval:'${algo.policy.shape_meta.action_dim}* ${algo.skill_block_size}'}

    vocab_size: 1000
    block_size: 16
    
    n_layer: 6
    n_head: 6
    n_embd: 384
    attn_pdrop: 0.1
    embd_pdrop: 0.1
    beam_size: 5 # value of k for top k sampling
    temperature: 1.0 # temperature for sampling
  image_encoder_factory:
    _target_: quest.algos.utils.rgb_modules.ResnetEncoder
    _partial_: true
    input_shape: ${algo.policy.shape_meta.image_shape}
    output_size: ${algo.obs_emb_dim}
    pretrained: false
    freeze: false
    remove_layer_num: 4
    no_stride: false
    language_fusion: 'none'
  proprio_encoder_factory:
    _target_: quest.algos.utils.mlp_proj.MLPProj
    input_size: ${algo.policy.shape_meta.proprio_shapes}
    output_size: ${algo.proprio_emb_dim}
    num_layers: 1
  # TODO: this assumes that all images have the same shape
  image_aug:
    _target_: quest.algos.utils.data_augmentation.DataAugGroup
    aug_list:
      - _target_: quest.algos.utils.data_augmentation.BatchWiseImgColorJitterAug
        input_shape: ${algo.policy.shape_meta.image_shape}
        brightness: 0.3
        contrast: 0.3
        saturation: 0.3
        hue: 0.3
        epsilon: 0.1
      - _target_: quest.algos.utils.data_augmentation.TranslationAug
        input_shape: ${algo.policy.shape_meta.image_shape}
        translation: 4
  shape_meta: null # This needs to be passed in when instantiating
  cat_obs_dim: ${eval:'${algo.obs_emb_dim}* ${algo.proprio_emb_dim}'}


train_dataloader:
  _target_: torch.utils.data.DataLoader
  batch_size: 256
  num_workers: 4
  sampler:
    _target_: torch.utils.data.RandomSampler
    data_source: ${algo.train_dataloader.dataset}
  persistent_workers: true
  pin_memory: true
  prefetch_factor: 2
  multiprocessing_context: fork




training:
  # training
  n_epochs: 100
  grad_clip: 100.
  loss_scale: 1.0
  loss_type: "l1"
  save_interval: 50

  # resume training
  resume: false
  resume_path: ""

optimizer:
  _target_: torch.optim.AdamW
  lr: 0.0001
  betas: [0.9, 0.999]    
  weight_decay: 0.0001

scheduler:
  _target_: torch.optim.lr_scheduler.CosineAnnealingLR
  eta_min: 1e-5
  last_epoch: -1
  T_max: ${algo.training.n_epochs}



# Put hyperparameters that are susceptible to change here
proprio_emb_dim: 128
obs_emb_dim: 256 # from resnet_out_dim to this value using MLP
offset_loss_scale: 10.0
mpc_horizon: 2 # mpc horizon for execution
skill_block_size: 32 # this is input sequence length to encoder